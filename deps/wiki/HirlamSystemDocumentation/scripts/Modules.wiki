[[PageOutline]]

[[Center(begin)]]
== '''Hirlam System Documentation''' ==
= '''Typical Run Time Error Messages Under mSMS =
[[Center(end)]]

== The Main HIRLAM suite == 

The following reasons for the system to fail have been observed: 

'''''Core and/or memory problems'''''

The HIRLAM system requires a large core and memory. Usually they are available on the computer system, but often you will have to increase the maximum allocation. The climate generation system is one of the first HIRLAM modules that is likely to hit the default allocation, but also other modules will do so. You will have to request more memory and core before you submit Hirlam. 
To avoid that you fall into the same trap again, it is best to increase the allocation by adding the following line in your .login file (if using the C shell): 
{{{
unlimit; limit coredumpsize 0
}}}
then source the .login file (type `source ~/.login'), and resume HIRLAM. In the Korn shell, use ulimit. 

Unfortunately, on many Linux installation only the super-user can request sufficient memory and core this way. Either redo the installation with bigger limits, or let the super-user write a setuid wrapper to allow users to request what they need. 

== Check user provided options ==

'''Domain'''

There are a number of limitations on model domain. The most important one is that NLON, the number of grid points in the West-East direction, must be even, and that NLON - 2 - 2 * NPBPTS must not have other prime factors than 2, 3 and 5. Here, NPBPTS is the number of `passive boundary points'. 

If the resolution is better than 0.045° (5 km), the environment variable HIRES must be set to a non-zero value. 

'''''Possible reasons for the system to fail'''''

The following reasons for the system to fail have been observed: 
{{{
A variable was not set 
}}}
Variables are read from the file with title $SETENV. Perhaps the variable was not set in this file. This will happen if the `Env_....' file that contains the variable is not executable. If it was not executable, make it executable and retry. 
  
== !InitRun ==

Module !InitRun initializes a HIRLAM run. Its main task is to make the HIRLAM system, including the modifications by the user, available to every HIRLAM task, on every host where it may be expected to be needed. It is required to run on the host where (mini-)SMS is run. This host must be named HOST0 in Env_system. 

There are several ideas at the basis of the structure of the HIRLAM suite. One of them is that each job that is submitted should not assume any environment. So each job must start by creating its own environment. For that, it may read files, e.g. Env_system. A consequence of this idea is that the name of file Env_system cannot be read from the environment. In stead, it must be a fixed name. However, InitRun itself is assumed to run in the same environment as (mini-)SMS. So it inherits e.g. the list of hosts to be used in the sequence of environment variables HOSTn, where n are integers, numbering the hosts consecutively, starting at 0. 

The first action is to make sure that the required directories exist on HOST0 (i.e. this host) and on the archiving system (HL_EXP). 

The !InitRun creates tar files of the user modifications, including progress.log. These, and Env_system, are copied to the directory HL_LIB on each host. From this point onwards, Env_system is treated as any other script. 

The files are first copied to HL_LIB on HOST0. The scripts are taken in the order of the default HIRLAM path, which is $HL_SCR:$hl_hs:$HL_WD. From this point onwards, Env_system is treated as any other script. Files to be copied are collected in tar files scripts.tar (for the scripts) and exp.tar (user modifications). 

The copying of the HIRLAM system and the user modifications to the other hosts is organised by a Perl script, InitRun.pl. This script reads the copy method from the environment variable RCP and the remote execution method from RSH (defaults rcp and rsh, resp.). 

!InitRun.pl executes a loop over all hosts, as found in the list HOSTn. In this loop, it extracts the required information (in particular target directories, but also e.g.) for each host from Env_system. It does that by executing Env_system where it sets the host to the corresponding host, while the variable FAKEHOST is set to indicate to Env_system that this is not necessarily the host executing Env_system (that is always HOST0), but that Env_system is only executed to extract the information for HOSTn, n≥0. 

The information extracted this way from Env_system is written in a (mini-)SMS header file, hostDescriptions.h. Because (mini-)SMS executes on HOST0, this file is only needed on HOST0. This file contains lines of the form 
{{{
ENV_VARn=VALUE export ENV_VARn
}}}
where ENV_VAR is the name of the required environment variable (e.g. HL_DATA), n is the host number, and VALUE the value of that variable. Also, the environment variables that the user suppied on the Hirlam command line, and that have been collected in the variable HL_CLA, are written to that header file. It is assumed that each subsequent job submitted by (mini-)SMS includes this header file hostDescriptions.h. 
!InitRun.pl proceeds by copying the tar files, created before, and Env_system, to the remote host using RCP. The tar files are un-tarred using RSH. 

'''''Possible reasons for the system to fail'''''

Could not ...

!InitRun.pl failed to copy and un-tar one or more files. Possibly, the user lacked the required permissions, e.g. to create directories or write files on a remote host. 

== The Climate Generation ==

The following reasons for the system to fail have been observed: 

'''''File not found'''''

Input files of orography and land-use are listed in files gtopo/gtopo30.lst and gtopo/glcc.lst. Normally, these files are taken from the HIRLAM reference system. 

'''''Core and/or memory problems'''''

The HIRLAM system requires a large core and memory. Usually they are available on the computer system, but often you will have to increase the maximum allocation. The climate generation system is one of the first HIRLAM modules that is likely to hit the default allocation. 
It is best to increase the allocation by adding the following line in your .login file (if using the C shell): 
{{{
unlimit; limit coredumpsize 0
}}}
then source the .login file (type `source ~/.login'), and resume HIRLAM. In the Korn shell, use ulimit. 

Unfortunately, on many Linux installation only the super-user can request sufficient memory and core this way. Either redo the installation with bigger limits, or let the super-user write a setuid wrapper to allow users to request what they need. 

== Fg (Preparation of the start data set for the cycle) ==

Usually, in an experiment, the analysis of a cycle uses a forecast of appropriate length from the previous cycle as the first-guess. The analysis may also need other files from the previous cycle (e.g. a list of black-listed stations or error statistics for Kalman filtering). The actions taken by the script Fg are to set up symbolic links to appropriate files. The first-guess file is called sds (start data set). 

The first cycle of an experiment has the peculiarity that there are no files from a previous cycle. For that cycle, other files will be used in stead. In particular, the first file with lateral boundaries will be used as sds. It is assumed that that file does not just contain data for the boundary zone, but also for the inner area (if it does not, the analysis and/or forecast will fail). 

The file $HL_WD/progress.log is used to determine whether the current cycle is the first in an experiment: if that file sets the variable , the cycle is deemed to be the first in an experiment. Also, if the file does not exist, or contains a date/time group earlier than the current one, the cycle is treated as the first one. 

'''''Possible reasons for the system to fail'''''

The following reasons for the system to fail have been observed: 
{{{
first guess name not found for cycle datetime
}}}
If the file $HL_WD/progress.log indicates that there has been a previous cycle, but the forecast from that cycle that is suitable as first-guess for the current cycle cannot be found, the script Fg will abort. 

Cure this by either: 

 * Construct a file to be used as first-guess, and put it in a subdirecotry of $HL_DATA and with a title, as indicated in the error message. 
 * Indicate in $HL_WD/progress.log that this is the first cycle. This can be done by adding one line to that file: 
{{{
first_cycle=1
}}}
 * Use Hirlam continue or Hirlam resume with the option first_cycle=1. Note that Hirlam start will mark the current cycle as the first one so with start this error report will not be issued. 

If you choose to mark this cycle as the first one, you should make sure that there is an alternative data set that can be used as first-guess. 
{{{
could not find a suitable start data set in directory 
}}}
This error message will be issued if on the first cycle of an experiment Fg was not able to find an alternative start data set. This implies it did neither find a first-guess from a previous cycle (even if this is a first cycle, Fg will anyhow try to continue from an earlier cycle); and it did not find a suitable file with lateral boundary conditions. 
Cure this by providing a suitable first-guess. 
 
== Forecast ==

The following reasons for the system to fail have been observed: 

'''''Exceeded CP time'''''

After your job consumed its allocations, it will fail. It may even not have the opportunity to report back to mini-SMS that it failed, in which case mini-SMS will keep the job in status `active'. If you think this has happened to your job, adjust the job, or increase the requested resources, and re-queue the job. 

Although the overrunning of allocations can happen to any job, the forecast model is particularly susceptive to this, because it consumes so many resources (and also because it may get dead-locked if you did not get the parallellisation right...). 

To help you in monitoring the progress of jobs with mini-SMS, you should try to place limits on resources in such a way that if the forecast model fails because it runs out of allocation, it still has some resources left to communicate its failure back to mini-SMS. For example, if you use qsub, try to use -lT to limit total allocation, and -lt to limit the allocation for the forcast model itself. If the -lt limit is slightly smaller than the -lT limit, the job will have some resources left after the forecast model hit the -lt limit, to send information about its failure back to mini-SMS. An example for a job requiring 7900 seconds of CP time could be: -lT 8000 -lt 7900. 

'''''No such file or directory, orography not found'''''

The message OROGRAPHY NOT FOUND - ABORT usually means that an attempt to read an input file (analysis, or lateral boundary conditions) failed. The message may mean that the file was corrupt, but most likely it means that the file did not exist at all. Then the next paragraph applies. But if the file existed it must have been corrupt, and then it should be recreated, by rerunning the analysis, or one of the families for lateral boundary conditions (named: LBCn), as appropriate. 

If an input file is missing, diagnosed e.g. by one of the following: 
{{{
../yyyymmdd_hh/namrunProg: No such file or directory 
}}}
then this may find its cause in a disk cleaning procedure, or a disk crash, between the moment that the file was created and the moment that the file was needed. This problem may occur both for the file itself, as for possibly required symbolic links related to the file. The solution is to recreate the input files. Do this by re-queuing all tasks in family FCinput. Then the forecast model can be re-queued as well. It is probably not adequate to simply restart mini-SMS in resume mode, because the FCinput family will be deemed to be complete, so it will not be rerun. 

'''''nprocy*nprocx # nproc'''''

A message of the type 
{{{
nprocy*nprocx # nproc:  nx * ny # n
}}}
is generated if the number of processors requested (n) is not equal to the product of the deemed number of processors in the x and y direction, resp. The following paragraphs describe where the (two) deemed and the (one) requested number of processors are determined. Do the necessary adjustments to make the three numbers consistent. 

The two deemed numbers of processors are read from the namelist, which is generated by the Execute task of the family FCinput. This script reads those numbers from the 'ENVIRONMENT' file which, in its turn, is constructed from the script Env_qsub. 

The way that the requested number of processors is specified depends on the operating system and the parallellisation paradigm. Under NQS, the number of processors is given on the qsub command line (e.g. with the -lP option). On some systems it may have to be passed on the command line (e.g. with the -n option of the mpprun command on T3E). 

'''''TRUNCATION IN HALO ZONE IN BIXINT'''''

A message of this type is issued if the length of a trajectory in the semi-Lagrangian advection scheme exceeds the width of the halo zone. The halo zone consists of the row of grid points of which the field values are exchanged between neighbouring processors at every time step. This exchange is organised by MPI. So this error can only occur on parallel implementations, not using shared memory. 

The cure consists of decreasing the time step (NDTIME in script Env_domain). If this would make the run unduly expensive, you may try to increase the width of the halo zone. You do the change in Env_domain for parameter GPHALO. 

'''''GRIBEX : End of message 7777 group not found'''''
 
The message: 
{{{
GRIBEX : End of message 7777 group not found. Return code = 805
ABORTX : Routine GRIBEX has requested program termination.
}}}

indicates that one of the input files was corrupt. Try to figure out which one (analysis, lateral boundary conditions) from the progress of the forecast model until this error was reported and recreate the offending file (see above). 

== Forecast model listener ==  

During its run, the HIRLAM forecast model writes lines to a file that reports the progress of the run. There will be one line for each model output file, written when the writing of that output file is complete. The line contains the instructions to further proces the corresponding file, in the form of the (U/Li)nix command line to be executed. 

Listen2Forecast inspects the file that reports the progress at regular intervals. Whenever it finds that a new line has been added to that file, it will execute the command as given in that line. In this way, it is possible to 'postproces' the model output files already during the forecast: without the need to wait for the model to complete, Listen2Forecast will proces the output files but only after the writing of the files is complete. Note that it would not have sufficed to merely test for the existence of the file, because the file may still be being written to. 

In older versions of the HIRLAM forecast model, the postprocessing command was issued by the model directly, through a system call from the Fortran code. This method is conceptually simpler, and hence preferable, but on at least one operating system it was not possible to launch more than 20 odd processes from the forecast model. The new method, with 'Listen2Forecast', does not have such limitations. On the other hand, to really start the postprocessing already during the run, it is needed that the file with the postprocessing commands be flushed after the writing of each line. This requires the presence of the (not-real, but de-facto) standard routine 'FLUSH', callable from Fortran. 

'''''Possible reasons for the system to fail'''''

The following reasons for the system to fail have been observed: 
{{{
The forecast model failed
}}}

If the forecast model fails on its last attempt, Listen2Forecast will also fail, mainly to remind you that it has to be resubmitted after you corrected the failure of the forecast model. 

Note that (mini-)SMS will assume the aborted status as long as there is at least one aborted job. In this case, even if you have cured the model problem that caused Listen2Forecast to fail, (mini-)SMS will continue to abort unless you change the status of Listen2Forecast to queued. 

== AnSFC ==
 

== AnUA ==
 

== !CollectLogs ==

Some families in HIRLAM are completed by a task to collect the standard output and standard error files into a larger file. That file is in HTML. Its title starts with HL_. 

There is one such file for the family Prepare, one for each cycle in the family Cycle, and one for each cycle in the family PPCycle. The File->show files menu produces a clickable list of the available files. 

'''''Possible reasons for the system to fail'''''

The following reasons for the system to fail have been observed: 
{{{
One of the earlier job failed
}}}
If one of the jobs of which the output is to be collected failed on its last attempt, CollectLogs will also fail, mainly to remind you that it has to be resubmitted after you corrected the failure of the other jobs. 
Note that (mini-)SMS will assume the aborted status as long as there is at least one aborted job. In this case, even if you have cured the problem with the job that caused CollectLogs to fail, (mini-)SMS will continue to abort unless you change the status of CollectLogs to queued. 
 

[[SubWiki]]

[[Center(begin)]]
[https://hirlam.org/trac/wiki/HirlamSystemDocumentation Back to the main page of the HIRLAM System Documentation]
[[Center(end)]]
----
[[Center(begin)]]
[[Disclaimer]]

[[Color(red, Last modified on)]] [[LastModified]]
[[Center(end)]]